{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpqrG51HzEAf"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*MauvaQb4aokes29WJ27OnQ.png\" width=\"300\" height=\"400\">\n",
    "\n",
    "# SQOOP: ingesta de datos (Puntuación: 50% de la PEC1, todos los ejercicios tienen el mismo valor)\n",
    "\n",
    "Como ya se ha visto en la parte de teoría el sistema SQOOP es una herramienta fundamental del entorno Big Data de Apache Hadoop. Sqoop nos permite llevar datos de bases de datos relacionales a HDFS, lo que conocemos como ingesta de datos. La ingesta de datos es el primer paso de cualquier proyecto Big Data, ya que antes de empezar a ver cómo almacenar y procesar los datos en los entornos Big Data lo primero que tenemos que hacer es ingestar datos en ellos. En esta práctica exploraremos cómo podemos interactuar desde la linea de comandos con SQOOP. Hemos preparado una Base de datos relacional MySQL con varias tablas, en los siguientes ejercicios interactuaremos con la BBDD, haremos algunas consultas a los datos y por último ingestaremos unos datos en HDFS.\n",
    "\n",
    "**Para ejecutar los ejercicios de SQOOP no podreis hacerlo desde el notebook, tendreis que hacerlo desde la terminal. Para abrir la terminal podeis hacerlo desde el icono del + arriba a la izquierda y despues en Others -> Terminal. Esta parte de la práctica la entregareis adjuntando capturas de pantalla de los comandos ejecutados y de el resultado de la ejecución en este mismo notebook**\n",
    "\n",
    "En el siguiente enlace podeis encontrar toda la información sobre los comandos para importar datos con SQOOP: https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_literal_sqoop_import_literal\n",
    "\n",
    "Para el bloque de ejericicios de SQOOP necesitareis conocimientos básicos de SQL, existe numerosa documentación online sobre SQL, por ejemplo en este tutorial podreis encontrar todo lo necesario para hacer estos ejercicios: https://www.tutorialspoint.com/sql/index.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kcKDE3tzEAi"
   },
   "source": [
    "En primer lugar, una vez esteis en la consola, teneis que acceder a la base de datos MySQL utilizando vuestro usuario y contraseña con el siguiente comando (una vez introduzcais el comando os pedirá que escribais la password que os hemos facilitado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIfNHlMmzEAj"
   },
   "outputs": [],
   "source": [
    "# mysql -u <arodriguezjus> -p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-AKoZnIzEAk"
   },
   "source": [
    "1. Utilizad la BBDD **DWH_RESERVAS** y listad las tablas que existen. La base de datos dispone de 1 tabla de hechos y 4 tablas de dimensiones, explorad las tablas y explicad como se relacionan entre ellas y qué información contienen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVeQO3VgzEAk"
   },
   "source": [
    "**Explicad en este espacio que tablas componen la BBDD y cómo se relacionan entre ellas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![](./img/1.png)\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "Las tablas se relacionan entre sí a través de las claves foráneas en la tabla de hechos h_reserva que contiene información sobre las reservas de habitaciones de hotel y se relaciona con las otras tablas a través de las claves foráneas id_pais, id_regimen e id_tipo_habitacion. Estas claves foráneas conectan las reservas con información adicional sobre el país, el régimen de alojamiento y el tipo de habitación, respectivamente y además con la tabla de hoteles a través de la clave foránea id_hotel, conectando las reservas con información adicional sobre el hotel asociado..\n",
    "\n",
    "**h_reserva** se relaciona con :\n",
    "\n",
    ">* **d_pais** a través del campo **id_pais**. *h_reserva.id_pais* es una clave foránea que hace referencia a *d_pais.id_pais*. Esto significa que cada reserva está asociada con un país específico en la tabla d_pais.  \n",
    ">\n",
    "> * **d_regimen** a través del campo **id_regimen**. *h_reserva.id_regimen* es una clave foránea que hace referencia a *d_regimen.id_regimen*. Esto significa que cada reserva está asociada con un régimen de alojamiento específico en la tabla *d_regimen*.\n",
    ">\n",
    "> * **d_tipo_habitacion** a través del campo **id_tipo_habitacion**. *h_reserva.id_tipo_habitacion* es una clave foránea que hace referencia a *d_tipo_habitacion.id_tipo_habitacion*. Esto significa que cada reserva está asociada con un tipo de habitación específico en la tabla *d_tipo_habitacion*.\n",
    ">\n",
    "> *  **d_hotel** a través del campo **id_hotel**. *h_reserva.id_hotel* es una clave foránea que hace referencia al campo id en la tabla de hoteles. Esto significa que cada reserva está asociada con un hotel específico en la tabla d_hotel.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv1O0NPlzEAk"
   },
   "source": [
    "  2. Consultad la tabla **h_reserva** limitando el numero de filas a 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![](./img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3OCsyiozEAl"
   },
   "source": [
    "  3. Queremos saber la descripción del tipo de habitaciones más reservadas. Realizad una query que devuelva la descripción de las habitaciones y el numero de veces que ha sido alquilada una habitación de ese tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjTnWDRYzEAm"
   },
   "source": [
    "  4. Cread un proceso Sqoop que traspase la información de la tabla de MySQL **d_pais** a la ruta de HDFS /user/ vuestro_user_name /d_pais. Tendreis que completar el comando que se indica a continuación y lanzarlo desde la consola (abrid una nueva consola para tener una ejecución limpia).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbDbsoXGzEAm"
   },
   "source": [
    "**Realizad la ejecución en una terminal, copiad en la celda anterior el comando y adjuntad una captura de pantalla en este espacio del notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOp7MhWOzEAm"
   },
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://localhost/DWH_RESERVAS \\\n",
    "  --username arodriguezjus \\\n",
    "  --password 8uu3WNMk \\\n",
    "  --table d_pais \\\n",
    "  --target-dir /user/arodriguezjus/d_pais \\\n",
    "  --m 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZj0TqNszEAn"
   },
   "source": [
    "Con Sqoop podemos hacer ingestas totales de una tabla como la que acabamos de hacer en el ejercicio 4, o ingestas parciales, sólo algunos campos o sólo algunas filas utilizando la clausula WHERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5cJq4NfzEAn"
   },
   "source": [
    "   5. Cread un proceso Sqoop que traspase la información las columnas **id_tipo_habitacion** y **desc_tipo_habitacion** de la tabla de MySQL **d_tipo_habitacion** a la ruta de HDFS /user/ vuestro_user_name /d_tipo_habitacion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaaeYeH9zEAn"
   },
   "source": [
    "**Realizad la ejecución en una terminal, copiad en la celda anterior el comando y adjuntad una captura de pantalla en este espacio del notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTd_fsmyzEAn"
   },
   "outputs": [],
   "source": [
    "#sqoop import --connect jdbc:mysql://localhost/DWH_RESERVAS <FILL IN> \n",
    "        \n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://localhost/DWH_RESERVAS \\\n",
    "  --username arodriguezjus \\\n",
    "  --password 8uu3WNMk \\\n",
    "  --table d_tipo_habitacion \\\n",
    "  --columns \"id_tipo_habitacion, desc_tipo_habitacion\" \\\n",
    "  --target-dir /user/arodriguezjus/d_tipo_habitacion \\\n",
    "  --m 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtoNeDJyzEAn"
   },
   "source": [
    "   6. Cread un proceso Sqoop que traspase la información de la tabla de MySQL **h_reserva** a la ruta de HDFS /user/ vuestro_user_name /h_reserva, pero sólo para los registros que cumplan **que el importe es mayor que 2000**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5ohN-dIzEAo"
   },
   "source": [
    "**Realizad la ejecución en una terminal, copiad en la celda anterior el comando y adjuntad una captura de pantalla en este espacio del notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGZk1Ta4zEAo"
   },
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://localhost/DWH_RESERVAS \\\n",
    "  --username arodriguezjus \\\n",
    "  --password 8uu3WNMk \\\n",
    "  --table h_reserva \\\n",
    "  --where \"importe > 2000\" \\\n",
    "  --target-dir /user/arodriguezjus/h_reserva \\\n",
    "  --num-mappers 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/7.png)\n",
    "\n",
    "################# continúa #################\n",
    "  \n",
    "![](./img/8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/arodriguezjus/h_reserva\n",
    "\n",
    "hdfs dfs -cat /user/arodriguezjus/h_reserva/part-m-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRxnKWvHzEAo"
   },
   "source": [
    "############################################################################### ###############################################################################\n",
    " \n",
    "  \n",
    " \n",
    "<img src=\"https://hadoop.apache.org/docs/r1.2.1/images/hadoop-logo.jpg\">\n",
    "\n",
    "# Sistema de ficheros HDFS: configuración del entorno de la asignatura (Puntuación: 50% de la PEC1, todos los ejercicios tienen el mismo valor)\n",
    "\n",
    "Como ya se ha visto en la parte de teoría el sistema de archivos Hadoop (HDFS) es una parte fundamental del entorno Big Data de Apache Hadoop. En esta práctica exploraremos cómo podemos interactuar desde la linea de comandos con el sistema de ficheros HDFS. El primer paso es abrir un terminal desde el JupyterLab. Una vez confeccionada correctamente el pedido mediante la terminal se ruega que ésta sea ejecutada dentro del entorno JupyterLab como enseña a continuación.\n",
    "\n",
    "Los comandos que se pueden enviar al sistema de archivos son muy similares a las de bash en entornos Linux. En la siguiente referencia oficial se pueden encontrar toda la información relativa a los comandos HDFS https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\n",
    "\n",
    "Los comandos de HDFS que utilizaremos siempre van a empezar por `hdfs dfs` seguimos del comando o comandos que queramos ejecutar, por ejemplo, para listar los archivos del directorio raíz del HDFS utilizaremos el comando *ls*. \n",
    "\n",
    "`hdfs dfs -ls /`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKEJ7nVqzEAo"
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zk7sugSzEAp"
   },
   "source": [
    "y para listar los ficheros del directorio /user: \n",
    "\n",
    "`hdfs dfs -ls /user`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xG92dVhpzEAp"
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlG__DFpzEAp"
   },
   "source": [
    "Tal y como se verá a continuación los comandos que HDFS proporciona, permiten: listar, consultar, subir o bajar archivos desde el directorio local en HDFS. Como ya se ha podido ver el aula contiene una serie de directorios donde se proporcionarán y se dejarán los archivos necesarios para la realización de la asignatura. A continuación se pide ejecutar una serie de operaciones que dejarán su directorio hdfs personal configurado para la realización de la asignatura. Puede encontrar el manual de los comandos del sistema HDFS en la parte de \"Contenidos y Recursos\" de la asignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuwqrTzpzEAp"
   },
   "source": [
    "En primer lugar vamos a empezar inspeccionando los ficheros que hemos ingestando con SQOOP en el ejercicio anterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOa4X9O6zEAp"
   },
   "source": [
    "1. Listar la carpeta */user/\\<loginestudiante\\>/d_pais* el directorio HDFS. ¿Qué ficheros aparecen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/arodriguezjus/d_pais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzXHJjFHzEAp"
   },
   "source": [
    "![](./img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXa5lctJzEAp"
   },
   "source": [
    "2. Inspeccionar el contenido del fichero */user/\\<loginestudiante\\>/d_pais/part-m-00000* de HDFS. Utilizad el comando `hdfs dfs -cat` ¿Qué información contiene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hbc5WExyzEAq"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/arodriguezjus/d_pais/part-m-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime un listado de países de la a hasta la z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4Y-V_VMzEAq"
   },
   "source": [
    "3. Crear la carpeta \"data\" dentro del directorio HDFS */user/\\<loginestudiante\\>/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUWvnorazEAq"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /user/arodriguezjus/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXyv30WQzEAq"
   },
   "source": [
    "4. Copiar al directorio \"data\" que acabais de crear el fichero \"/user/\\<loginestudiante\\>/d_pais/part-m-00000\" que hemos inspeccionado anteriormente. Utilitzad el comando `hdfs dfs -cp` para copiar el fichero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3CaEqoZzEAq"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cp /user/arodriguezjus/d_pais/part-m-00000 /user/arodriguezjus/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrGI_bVLzEAq"
   },
   "source": [
    "5. Copiar las 10 primeras lineas del fichero \"LoremIpsum.txt\" que se encuentra en el directorio HDFS */aula_M2.858/data/* a un nuevo fichero \"LoremIpsum_10.txt\" que creareis en el directorio HDFS */user/\\<loginestudiante\\>/data*. Os seran de utilidad los comandos `hdfs dfs -cat` i `hdfs dfs -put` a parte del comando classico de bash `head`. A continuación utilizad el comando `cat` para comprobar que se ha realizado la acción correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKW3sLPVzEAq"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cat /aula_M2.858/data/LoremIpsum.txt | head -n 10 > LoremIpsum_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hldA1AZ8zEAr"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -put LoremIpsum_10.txt /user/arodriguezjus/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/arodriguezjus/data/LoremIpsum_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnHgtk3KzEAr"
   },
   "source": [
    "6. Descargar el fichero que acabais de crear */user/\\<loginestudiante\\>/data/LoremIpsum_10.txt* . Utilizar el comando `hdfs dfs -get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhWPmPbZzEAr"
   },
   "outputs": [],
   "source": [
    "hdfs dfs -get /user/arodriguezjus/data/LoremIpsum_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SybC-eKizEAr"
   },
   "source": [
    "A parte de los comandos que ya habéis visto orientados a que cada usuario pueda gestionar el sistema de fitxeros HDFS, también es posible explorar la organización interna de los diferentes fitxeros existentes en los directorios HDFS.\n",
    "\n",
    "7. Se pide que mediante el comando `hdfs fsck` se determine i describa como esta estructurado el fichero */user/\\<loginestudiante\\>/data/LoremIpsum_10.txt* dentro del sistema HDFS. En concreto se pide mostrar como está subdividido en bloques, en que nodo están los bloques y si estos estéan replicados. Podeis consultar està información en la web oficial de referéncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_e8eB8rzEAr"
   },
   "outputs": [],
   "source": [
    "hdfs fsck /user/arodriguezjus/data/LoremIpsum_10.txt -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">El archivo se divide en un bloque de tamaño 935 bytes. Este bloque se encuentra replicado en tres datanodes: 10.20.30.3, 10.20.30.2 y 213.73.35.120. Además, se puede observar que no hay bloques faltantes, ni corrupción de bloques, ni bloques faltantes por replicar o por reemplazar. Por lo tanto, la integridad del archivo es correcta y está disponible para ser utilizado en el sistema HDFS.<span>\n",
    "\n",
    "![](./img/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyLqfMs3zEAr"
   },
   "source": [
    "8. Como sabeis los diferentes ficheros HDFS están particionados en bloques. Podeis consultar la configuración mediante el comando: `hdfs getconf -confKey dfs.blocksize`. Quantos MB hay por bloque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYV4eTuozEAr"
   },
   "outputs": [],
   "source": [
    "hdfs getconf -confKey dfs.blocksize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ShWRUMbzU2U"
   },
   "source": [
    "9. ¿Si un fichero tuviera un tamaño de 340MB y lo almacenaramos en nuestro sistema HDFS, cuantos bloques se habrían generado (sin contar réplicas) y de qué tamaño cada una?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twhSteIlzU2U"
   },
   "outputs": [],
   "source": [
    "# El tamaño de bloque por defecto en HDFS es de 128 MB, \n",
    "# por lo que un archivo de 340 MB generaría 3 bloques, \n",
    "# donde los primeros dos bloques tendrían un tamaño de 128 MB cada uno \n",
    "# y el tercer bloque tendría un tamaño de 84 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6AXLD96zU2V"
   },
   "source": [
    "10. ¿Qué es la tolerancia a fallos y cómo se consigue en HDFS?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJyp5kwIzU2V"
   },
   "outputs": [],
   "source": [
    "# La tolerancia a fallos se refiere a la capacidad de un sistema para continuar funcionando \n",
    "# en caso de fallos de hardware o software. En el caso de HDFS, la tolerancia a fallos se logra \n",
    "# mediante la replicación de los bloques de datos a través de múltiples nodos en un clúster. \n",
    "# Cada bloque se replica en al menos tres nodos diferentes, lo que garantiza que si uno o dos nodos \n",
    "# fallan, los datos todavía están disponibles en el clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wy3HTIJzU2V"
   },
   "source": [
    "11. Se pide explorar la estructura del sistema HDFS mediante su WebUI. Para ello primeramente debemos creare un túnel ssh que nos dé acceso al puerto 9870 (sobre el que está corriendo la Web UI de HDFS). A los alumnos que tengáis un sistema basado en Linux, podéis hacerlo de manera muy simple mediante el siguiente comando:\n",
    "\n",
    "```python\n",
    "ssh -p55000  -L 9870:eimtcld2.uoclabs.uoc.es:9870 <login>@eimtcld2.uoclabs.uoc.es\n",
    "       \n",
    "```\n",
    "Una vez ejecutado el comando y logeado correctamente podéis acceder a la Web UI de HDFS en la dirección:\n",
    "\n",
    "```Python\n",
    "http://localhost:9870/dfshealth.html#tab-overview\n",
    "```\n",
    "\n",
    "> Nota: a los alumnos que solo dispongan de un sistema Windows, adjuntamos una pequeña descripción para crear el túnel ssh mediante la aplicación Putty.\n",
    "\n",
    "Esta pregunta es de texto libre y se pide que se describa a continuación qué información podéis obtener explorando la Web UI, número de DataNodes, localización de estos y su estado, factores de replicación, errores si los hay, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mrjXzfNzU2V"
   },
   "outputs": [],
   "source": [
    "#Escriba aqui la respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">La Web UI nos proporciona una gran cantidad de información sobre el estado del sistema HDFS, incluyendo entre otros puntos:<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Resumen**: un resumen del estado actual del sistema, incluyendo información sobre la seguridad, el modo seguro y el número total de objetos del sistema de archivos, como archivos y directorios, etc.<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "   \n",
    "* **Número de DataNodes**: en el apartado \"Live Nodes\" podemos ver el número total de nodos de datos (DataNodes) que están actualmente conectados al sistema HDFS. En este caso, hay 3 nodos vivos.<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Localización de los DataNodes**: en la sección \"Live Nodes\" podemos ver la ubicación física de los DataNodes conectados actualmente, incluyendo su dirección IP y el puerto en el que están escuchando.<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Estado de los DataNodes**: en la sección \"Live Nodes\" también se muestra el estado actual de cada DataNode, es decir, si está en funcionamiento o no, junto a su dirección.<span>\n",
    " ![](./img/15.png)\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Factores de replicación**: la Web UI muestra información sobre la cantidad de bloques replicados y el número de bloques sub-replicados (es decir, aquellos que no se han replicado el número mínimo de veces). En particular, en la columna \"Blocks\" se muestra el número de bloques que tiene cada datanode y en la columna \"Block pool used\" se muestra la cantidad de espacio utilizado por los bloques replicados. Por lo tanto, se puede deducir el factor de replicación dividiendo el número total de bloques replicados por el número total de bloques.<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Errores**: si hay algún error en el sistema, la Web UI lo muestra en la página principal y proporciona información detallada sobre el problema.<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Espacio utilizado**: Se muestra la cantidad de espacio utilizado por el sistema HDFS, tanto por el sistema de archivos DFS como por los datos que no pertenecen a DFS.<span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "* **Histograma de uso de DataNode**: Se muestra la cantidad de espacio utilizado por cada DataNode en un histograma.\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/16.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
